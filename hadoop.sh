#!/bin/bash

#=============================================================================
# Hadoop é›†ç¾¤è‡ªåŠ¨éƒ¨ç½²è„šæœ¬ (è™šæ‹Ÿæœºç‰ˆæœ¬) - ä¿®å¤ç‰ˆ
# é€‚ç”¨äº: Ubuntu 24.04 + Hadoop 3.4.2 + OpenJDK 17
# ä½¿ç”¨æ–¹æ³•: bash hadoop_deploy.sh
#=============================================================================

set -e # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# å…¨å±€å˜é‡
HADOOP_VERSION="3.4.2"
HADOOP_USER="hadoop"
HADOOP_PASSWORD=""
CURRENT_USER=""
CURRENT_USER_PASSWORD=""
MASTER_IP=""
MASTER_HOSTNAME="hadoop01"
SLAVE_COUNT=0
declare -a SLAVE_IPS
declare -a SLAVE_HOSTNAMES

#=============================================================================
# å·¥å…·å‡½æ•°
#=============================================================================

print_info() {
  echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
  echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
  echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
  echo -e "${RED}[ERROR]${NC} $1"
}

print_step() {
  echo -e "\n${GREEN}========================================${NC}"
  echo -e "${GREEN}$1${NC}"
  echo -e "${GREEN}========================================${NC}\n"
}

print_progress() {
  echo -e "${CYAN}[è¿›åº¦]${NC} $1"
}

# åœ¨è¿œç¨‹èŠ‚ç‚¹æ‰§è¡Œå‘½ä»¤ - ä¿®å¤ç‰ˆ
exec_remote() {
  local host=$1
  local cmd=$2
  local user=$3
  local pwd=$4

  if [[ "$cmd" == *"sudo"* ]]; then
    sshpass -p "$pwd" ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 ${user}@${host} "echo '$pwd' | sudo -S bash -c '$cmd'"
  else
    sshpass -p "$pwd" ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 ${user}@${host} "$cmd"
  fi
}

# å‘è¿œç¨‹èŠ‚ç‚¹å¤åˆ¶æ–‡ä»¶
copy_to_remote() {
  local host=$1
  local src=$2
  local dest=$3
  local pwd=$4
  local user=$5
  sshpass -p "$pwd" rsync -avz --progress -e "ssh -o StrictHostKeyChecking=no" "$src" ${user}@${host}:"$dest"
}

# è¯¢é—®ç”¨æˆ·é€‰æ‹©
ask_user_choice() {
  echo -e "${YELLOW}$prompt${NC}"
  echo "  1) é‡æ–°å®‰è£… (reinstall)"
  echo "  2) è·³è¿‡æ­¤æ­¥éª¤ (skip)"
  echo "  3) é€€å‡ºè„šæœ¬ï¼Œæ‰‹åŠ¨è°ƒè¯• (exit)"
}

# æ˜¾ç¤ºè¿›åº¦æ¡
show_progress() {
  local current=$1
  local total=$2
  local task=$3
  local percent=$((current * 100 / total))
  local completed=$((current * 50 / total))
  local remaining=$((50 - completed))

  printf "\r${CYAN}[%3d%%]${NC} [" $percent
  printf "%${completed}s" | tr ' ' '='
  printf "%${remaining}s" | tr ' ' '-'
  printf "] %s" "$task"
}

#=============================================================================
# ç”¨æˆ·è¾“å…¥æ”¶é›†
#=============================================================================

collect_cluster_info() {
  print_step "æ­¥éª¤ 1: æ”¶é›†é›†ç¾¤ä¿¡æ¯"

  # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
  CURRENT_USER=$(whoami)
  print_info "å½“å‰ç”¨æˆ·: $CURRENT_USER"

  # è·å–å½“å‰ç”¨æˆ·å¯†ç ï¼ˆç”¨äºSSHåˆ°å…¶ä»–èŠ‚ç‚¹ï¼‰
  echo -n "è¯·è¾“å…¥å½“å‰ç”¨æˆ·($CURRENT_USER)åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„å¯†ç : "
  read -s CURRENT_USER_PASSWORD
  echo ""

  # è·å–MasterèŠ‚ç‚¹IP
  echo -n "è¯·è¾“å…¥MasterèŠ‚ç‚¹çš„IPåœ°å€: "
  read MASTER_IP

  # è·å–SlaveèŠ‚ç‚¹æ•°é‡
  echo -n "è¯·è¾“å…¥SlaveèŠ‚ç‚¹æ•°é‡ (å»ºè®®2-10ä¸ª): "
  read SLAVE_COUNT

  if [ "$SLAVE_COUNT" -lt 1 ]; then
    print_error "è‡³å°‘éœ€è¦1ä¸ªSlaveèŠ‚ç‚¹"
    exit 1
  fi

  # è·å–æ¯ä¸ªSlaveèŠ‚ç‚¹çš„IP
  for i in $(seq 1 $SLAVE_COUNT); do
    local num=$(printf "%02d" $((i + 1)))
    echo -n "è¯·è¾“å…¥SlaveèŠ‚ç‚¹ $i çš„IPåœ°å€: "
    read slave_ip
    SLAVE_IPS+=("$slave_ip")
    SLAVE_HOSTNAMES+=("hadoop${num}")
  done

  # è·å–hadoopç”¨æˆ·å¯†ç 
  echo -n "è¯·è¾“å…¥è¦ä¸ºhadoopç”¨æˆ·è®¾ç½®çš„å¯†ç  (æ‰€æœ‰èŠ‚ç‚¹ä½¿ç”¨ç›¸åŒå¯†ç ): "
  read -s HADOOP_PASSWORD
  echo ""

  # ç¡®è®¤ä¿¡æ¯
  echo -e "\n${YELLOW}=== é›†ç¾¤é…ç½®ç¡®è®¤ ===${NC}"
  echo "å½“å‰æ“ä½œç”¨æˆ·: $CURRENT_USER"
  echo "MasterèŠ‚ç‚¹: $MASTER_HOSTNAME ($MASTER_IP)"
  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    echo "SlaveèŠ‚ç‚¹ $((i + 1)): ${SLAVE_HOSTNAMES[$i]} (${SLAVE_IPS[$i]})"
  done
  echo ""
  echo -n "ç¡®è®¤ä»¥ä¸Šä¿¡æ¯æ­£ç¡®? (yes/no): "
  read confirm
  if [ "$confirm" != "yes" ]; then
    print_error "éƒ¨ç½²å·²å–æ¶ˆ"
    exit 1
  fi
}

#=============================================================================
# æ£€æŸ¥èŠ‚ç‚¹è¿æ¥æ€§
#=============================================================================

check_node_connectivity() {
  print_step "æ­¥éª¤ 2: æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹è¿æ¥æ€§"

  local total_nodes=$((SLAVE_COUNT + 1))
  local current=0

  # æ£€æŸ¥Master
  current=$((current + 1))
  show_progress $current $total_nodes "æ£€æŸ¥ Master èŠ‚ç‚¹ ($MASTER_IP)"
  if sshpass -p "$CURRENT_USER_PASSWORD" ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 ${CURRENT_USER}@${MASTER_IP} "echo ok" &>/dev/null; then
    echo ""
    print_success "Master èŠ‚ç‚¹è¿æ¥æ­£å¸¸"
  else
    echo ""
    print_error "æ— æ³•è¿æ¥åˆ° Master èŠ‚ç‚¹ $MASTER_IP"
    exit 1
  fi

  # æ£€æŸ¥æ‰€æœ‰Slave
  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    current=$((current + 1))
    show_progress $current $total_nodes "æ£€æŸ¥ Slave èŠ‚ç‚¹ ${SLAVE_HOSTNAMES[$i]} (${SLAVE_IPS[$i]})"
    if sshpass -p "$CURRENT_USER_PASSWORD" ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 ${CURRENT_USER}@${SLAVE_IPS[$i]} "echo ok" &>/dev/null; then
      echo ""
      print_success "${SLAVE_HOSTNAMES[$i]} èŠ‚ç‚¹è¿æ¥æ­£å¸¸"
    else
      echo ""
      print_error "æ— æ³•è¿æ¥åˆ° Slave èŠ‚ç‚¹ ${SLAVE_IPS[$i]}"
      exit 1
    fi
  done

  echo ""
  print_success "æ‰€æœ‰èŠ‚ç‚¹è¿æ¥æ£€æŸ¥å®Œæˆ"
}

#=============================================================================
# é…ç½®å•ä¸ªèŠ‚ç‚¹
#=============================================================================

configure_single_node() {
  local node_ip=$1
  local hostname=$2
  local is_master=$3
  local node_label="[$hostname]"

  print_info "$node_label å¼€å§‹åŸºç¡€é…ç½®"

  # =====================================================
  # 1. æ£€æŸ¥å¹¶åˆ›å»º hadoop ç”¨æˆ·
  # =====================================================
  print_progress "$node_label æ£€æŸ¥ hadoop ç”¨æˆ·..."

  local user_exists=$(sshpass -p "$CURRENT_USER_PASSWORD" ssh -o StrictHostKeyChecking=no ${CURRENT_USER}@${node_ip} \
    "id '$HADOOP_USER' &>/dev/null && echo 'yes' || echo 'no'")

  if [[ "$user_exists" == "yes" ]]; then
    print_success "$node_label hadoop ç”¨æˆ·å·²å­˜åœ¨"

    # å…ˆè°ƒç”¨å‡½æ•°æ˜¾ç¤ºæç¤ºä¿¡æ¯
    ask_user_choice "$node_label hadoop ç”¨æˆ·å·²å­˜åœ¨ï¼Œæ˜¯å¦é‡æ–°é…ç½®?"

    # è¯»å–ç”¨æˆ·è¾“å…¥åˆ°æœ¬åœ°å˜é‡
    echo -n "è¯·è¾“å…¥é€‰æ‹© [1/2/3] (é»˜è®¤ 2): "
    read -r user_input
    user_input=${user_input:-2} # é»˜è®¤é€‰æ‹© 2ï¼ˆè·³è¿‡ï¼‰

    # æ ¹æ®æ•°å­—è®¾ç½®åŠ¨ä½œ
    case "$user_input" in
    1) choice="reinstall" ;;
    2) choice="skip" ;;
    3) choice="exit" ;;
    *) choice="skip" ;; # å…¶ä»–è¾“å…¥ä¹Ÿé»˜è®¤è·³è¿‡
    esac

    # æ ¹æ® choice æ‰§è¡Œæ“ä½œ
    case "$choice" in
    "exit")
      print_warning "ç”¨æˆ·é€‰æ‹©é€€å‡º"
      exit 0
      ;;
    "reinstall")
      print_info "$node_label é‡æ–°é…ç½® hadoop ç”¨æˆ·"
      exec_remote "$node_ip" "sudo userdel -r $HADOOP_USER 2>/dev/null || true" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
      exec_remote "$node_ip" "sudo useradd -m -s /bin/bash '$HADOOP_USER' && echo '$HADOOP_USER:$HADOOP_PASSWORD' | sudo chpasswd && sudo usermod -aG sudo '$HADOOP_USER'" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
      ;;
    "skip")
      print_info "$node_label è·³è¿‡ hadoop ç”¨æˆ·é…ç½®"
      ;;
    esac
  else
    print_info "$node_label åˆ›å»º hadoop ç”¨æˆ·"
    exec_remote "$node_ip" "sudo useradd -m -s /bin/bash '$HADOOP_USER'" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    exec_remote "$node_ip" "echo '$HADOOP_USER:$HADOOP_PASSWORD' | sudo chpasswd" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    exec_remote "$node_ip" "sudo usermod -aG sudo '$HADOOP_USER'" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    print_success "$node_label hadoop ç”¨æˆ·åˆ›å»ºå®Œæˆ"
  fi

  # =====================================================
  # 2. è®¾ç½®ä¸»æœºå
  # =====================================================
  print_progress "$node_label æ£€æŸ¥ä¸»æœºå..."

  local current_hostname=$(sshpass -p "$CURRENT_USER_PASSWORD" ssh -o StrictHostKeyChecking=no ${CURRENT_USER}@${node_ip} "hostname")

  if [[ "$current_hostname" != "$hostname" ]]; then
    print_info "$node_label æ›´æ–°ä¸»æœºå: $current_hostname -> $hostname"
    exec_remote "$node_ip" "sudo hostnamectl set-hostname '$hostname'" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    print_success "$node_label ä¸»æœºåæ›´æ–°å®Œæˆ"
  else
    print_success "$node_label ä¸»æœºåå·²æ­£ç¡®: $hostname"
  fi

  # =====================================================
  # 3. å®‰è£…åŸºç¡€è½¯ä»¶
  # =====================================================
  print_progress "$node_label æ£€æŸ¥å¿…è¦è½¯ä»¶..."

  local missing_packages=""
  for pkg in wget ssh sshpass vim net-tools rsync; do
    local installed=$(sshpass -p "$CURRENT_USER_PASSWORD" ssh -o StrictHostKeyChecking=no ${CURRENT_USER}@${node_ip} \
      "dpkg -l | grep -w $pkg &>/dev/null && echo 'yes' || echo 'no'")
    if [[ "$installed" != "yes" ]]; then
      missing_packages="$missing_packages $pkg"
    fi
  done

  if [[ -n "$missing_packages" ]]; then
    print_info "$node_label å®‰è£…ç¼ºå¤±çš„è½¯ä»¶åŒ…:$missing_packages"
    exec_remote "$node_ip" "sudo apt-get update -y" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    exec_remote "$node_ip" "sudo apt-get install -y $missing_packages" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    print_success "$node_label è½¯ä»¶åŒ…å®‰è£…å®Œæˆ"
  else
    print_success "$node_label æ‰€æœ‰å¿…è¦è½¯ä»¶å·²å®‰è£…"
  fi

  # =====================================================
  # 4. å¯åŠ¨ SSH æœåŠ¡
  # =====================================================
  print_progress "$node_label é…ç½® SSH æœåŠ¡..."
  exec_remote "$node_ip" "sudo systemctl enable ssh && sudo systemctl restart ssh" "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
  print_success "$node_label SSH æœåŠ¡å·²å¯åŠ¨"

  # =====================================================
  # 5. æ£€æµ‹ JDK å®‰è£…çŠ¶æ€
  # =====================================================
  print_progress "$node_label æ£€æŸ¥ JDK å®‰è£…çŠ¶æ€..."

  local jdk_installed=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${node_ip} \
    "[ -d /usr/lib/jvm/jdk-17.0.12-oracle-x64 ] && java -version 2>&1 | grep -q '17.0.12' && echo 'yes' || echo 'no'")

  if [[ "$jdk_installed" == "yes" ]]; then
    print_success "$node_label JDK 17.0.12 å·²å®‰è£…"

    # è°ƒç”¨å‡½æ•°æ˜¾ç¤ºæç¤ºä¿¡æ¯
    ask_user_choice "$node_label JDKå·²å®‰è£…ï¼Œæ˜¯å¦é‡æ–°å®‰è£…?"

    # è¯»å–ç”¨æˆ·è¾“å…¥åˆ°æœ¬åœ°å˜é‡
    echo -n "è¯·è¾“å…¥é€‰æ‹© [1/2/3] (é»˜è®¤ 2): "
    read -r user_input
    user_input=${user_input:-2} # é»˜è®¤é€‰æ‹© 2ï¼ˆè·³è¿‡ï¼‰

    # æ•°å­—æ˜ å°„æˆåŠ¨ä½œ
    case "$user_input" in
    1) choice="reinstall" ;;
    2) choice="skip" ;;
    3) choice="exit" ;;
    *) choice="skip" ;; # å…¶ä»–è¾“å…¥é»˜è®¤è·³è¿‡
    esac

    # æ‰§è¡ŒåŠ¨ä½œ
    case "$choice" in
    "exit")
      print_warning "ç”¨æˆ·é€‰æ‹©é€€å‡º"
      exit 0
      ;;
    "reinstall")
      print_info "$node_label é‡æ–°å®‰è£… JDK"
      jdk_installed="no"
      ;;
    "skip")
      print_info "$node_label è·³è¿‡ JDK å®‰è£…"
      ;;
    esac
  fi

  # =====================================================
  # 6. å®‰è£… JDK
  # =====================================================
  if [[ "$jdk_installed" == "no" ]]; then
    local dir="$(dirname "$0")"
    local jdk_file="$dir/jdk-17.0.12_linux-x64_bin.deb"

    if [[ ! -f "$jdk_file" ]]; then
      print_error "$node_label æœªæ‰¾åˆ° JDK å®‰è£…åŒ…: $jdk_file"
      print_error "è¯·ä¸‹è½½ jdk-17.0.12_linux-x64_bin.deb å¹¶æ”¾åœ¨è„šæœ¬åŒç›®å½•ä¸‹"
      exit 1
    fi

    print_info "$node_label åˆ†å‘ JDK å®‰è£…åŒ…..."
    sshpass -p "$HADOOP_PASSWORD" rsync -avz --progress -e "ssh -o StrictHostKeyChecking=no" \
      "$jdk_file" ${HADOOP_USER}@${node_ip}:/tmp/ 2>&1 | grep -v "sending incremental" || true

    print_info "$node_label å®‰è£… JDK..."
    exec_remote "$node_ip" "sudo dpkg -i /tmp/jdk-17.0.12_linux-x64_bin.deb" "$HADOOP_USER" "$HADOOP_PASSWORD"

    print_info "$node_label é…ç½® JDK ç¯å¢ƒå˜é‡..."
    exec_remote "$node_ip" "grep -q 'JAVA_HOME.*jdk-17' ~/.bashrc || echo 'export JAVA_HOME=/usr/lib/jvm/jdk-17.0.12-oracle-x64' >> ~/.bashrc" "$HADOOP_USER" "$HADOOP_PASSWORD"
    exec_remote "$node_ip" "grep -q 'PATH.*JAVA_HOME' ~/.bashrc || echo 'export PATH=\$PATH:\$JAVA_HOME/bin' >> ~/.bashrc" "$HADOOP_USER" "$HADOOP_PASSWORD"

    print_success "$node_label JDK å®‰è£…å®Œæˆ"
  fi

  # =====================================================
  # 7. æ£€æµ‹ Hadoop å®‰è£…çŠ¶æ€
  # =====================================================
  print_progress "$node_label æ£€æŸ¥ Hadoop å®‰è£…çŠ¶æ€..."

  local hadoop_installed=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${node_ip} \
    "[ -d /usr/local/hadoop ] && [ -f /usr/local/hadoop/bin/hadoop ] && echo 'yes' || echo 'no'")

  if [[ "$hadoop_installed" == "yes" ]]; then
    print_success "$node_label Hadoop å·²å®‰è£…"

    if [[ "$is_master" != "true" ]]; then
      # è°ƒç”¨å‡½æ•°æ˜¾ç¤ºæç¤ºä¿¡æ¯
      ask_user_choice "$node_label Hadoopå·²å®‰è£…ï¼Œæ˜¯å¦é‡æ–°å®‰è£…?"

      # æœ¬åœ°è¯»å–ç”¨æˆ·è¾“å…¥
      echo -n "è¯·è¾“å…¥é€‰æ‹© [1/2/3] (é»˜è®¤ 2): "
      read -r user_input
      user_input=${user_input:-2} # é»˜è®¤é€‰æ‹© 2ï¼ˆè·³è¿‡ï¼‰

      # æ•°å­—æ˜ å°„æˆåŠ¨ä½œ
      case "$user_input" in
      1) choice="reinstall" ;;
      2) choice="skip" ;;
      3) choice="exit" ;;
      *) choice="skip" ;; # å…¶ä»–è¾“å…¥é»˜è®¤è·³è¿‡
      esac

      # æ‰§è¡ŒåŠ¨ä½œ
      case "$choice" in
      "exit")
        print_warning "ç”¨æˆ·é€‰æ‹©é€€å‡º"
        exit 0
        ;;
      "reinstall")
        print_info "$node_label å°†åœ¨åç»­æ­¥éª¤é‡æ–°å®‰è£… Hadoop"
        exec_remote "$node_ip" "sudo rm -rf /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"
        ;;
      "skip")
        print_info "$node_label è·³è¿‡ Hadoop å®‰è£…"
        ;;
      esac
    fi
  else
    print_info "$node_label Hadoop æœªå®‰è£…ï¼Œå°†åœ¨åç»­æ­¥éª¤å®‰è£…"
  fi

  print_success "$node_label èŠ‚ç‚¹åŸºç¡€é…ç½®å®Œæˆ"
}

#=============================================================================
# æ‰€æœ‰èŠ‚ç‚¹çš„åŸºç¡€é…ç½®
#=============================================================================

configure_all_nodes() {
  print_step "æ­¥éª¤ 3: é…ç½®æ‰€æœ‰èŠ‚ç‚¹çš„åŸºç¡€ç¯å¢ƒ"

  local total_nodes=$((SLAVE_COUNT + 1))
  local current=0

  # é…ç½® Master èŠ‚ç‚¹
  current=$((current + 1))
  show_progress $current $total_nodes "é…ç½® Master èŠ‚ç‚¹"
  echo ""
  configure_single_node "$MASTER_IP" "$MASTER_HOSTNAME" "true"

  # é…ç½®æ‰€æœ‰ Slave èŠ‚ç‚¹
  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    current=$((current + 1))
    show_progress $current $total_nodes "é…ç½® Slave èŠ‚ç‚¹ ${SLAVE_HOSTNAMES[$i]}"
    echo ""
    configure_single_node "${SLAVE_IPS[$i]}" "${SLAVE_HOSTNAMES[$i]}" "false"
  done

  echo ""
  print_success "æ‰€æœ‰èŠ‚ç‚¹åŸºç¡€é…ç½®å®Œæˆ"
}

#=============================================================================
# é…ç½®hostsæ–‡ä»¶
#=============================================================================

configure_hosts_file() {
  print_step "æ­¥éª¤ 4: é…ç½®æ‰€æœ‰èŠ‚ç‚¹çš„ hosts æ–‡ä»¶"

  # ç”Ÿæˆhostså†…å®¹
  local hosts_content="127.0.0.1 localhost"$'\n'"$MASTER_IP $MASTER_HOSTNAME"
  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    hosts_content+=$'\n'"${SLAVE_IPS[$i]} ${SLAVE_HOSTNAMES[$i]}"
  done

  local total_nodes=$((SLAVE_COUNT + 1))
  local current=0

  # æ›´æ–° Master èŠ‚ç‚¹
  current=$((current + 1))
  show_progress $current $total_nodes "æ›´æ–° Master èŠ‚ç‚¹ hosts"
  exec_remote "$MASTER_IP" "
        tmp_file=\$(mktemp)
        echo \"$hosts_content\" > \$tmp_file
        sudo mv \$tmp_file /etc/hosts
        sudo chmod 644 /etc/hosts
    " "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
  echo ""

  # æ›´æ–°æ‰€æœ‰ Slave èŠ‚ç‚¹
  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    current=$((current + 1))
    show_progress $current $total_nodes "æ›´æ–° ${SLAVE_HOSTNAMES[$i]} hosts"
    exec_remote "${SLAVE_IPS[$i]}" "
            tmp_file=\$(mktemp)
            echo \"$hosts_content\" > \$tmp_file
            sudo mv \$tmp_file /etc/hosts
            sudo chmod 644 /etc/hosts
        " "$CURRENT_USER" "$CURRENT_USER_PASSWORD"
    echo ""
  done

  print_success "æ‰€æœ‰èŠ‚ç‚¹ hosts æ–‡ä»¶é…ç½®å®Œæˆ"
}

#=============================================================================
# é…ç½®SSHæ— å¯†ç ç™»å½•
#=============================================================================

configure_ssh_keys() {
  print_step "æ­¥éª¤ 5: é…ç½® SSH æ— å¯†ç ç™»å½•"

  print_info "åœ¨ Master èŠ‚ç‚¹ç”Ÿæˆ SSH å¯†é’¥"
  exec_remote "$MASTER_IP" "mkdir -p ~/.ssh && chmod 700 ~/.ssh" "$HADOOP_USER" "$HADOOP_PASSWORD"
  exec_remote "$MASTER_IP" "[ -f ~/.ssh/id_rsa ] || ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa -q" "$HADOOP_USER" "$HADOOP_PASSWORD"

  print_info "è·å– Master å…¬é’¥"
  local master_pubkey=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${MASTER_IP} "cat ~/.ssh/id_rsa.pub")

  print_info "é…ç½® Master åˆ°è‡ªå·±çš„æ— å¯†ç ç™»å½•"
  exec_remote "$MASTER_IP" "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys" "$HADOOP_USER" "$HADOOP_PASSWORD"

  local total_slaves=$SLAVE_COUNT
  local current=0

  # é…ç½®æ‰€æœ‰ Slave èŠ‚ç‚¹
  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    current=$((current + 1))
    show_progress $current $total_slaves "é…ç½® ${SLAVE_HOSTNAMES[$i]} SSH"

    exec_remote "${SLAVE_IPS[$i]}" "mkdir -p ~/.ssh && chmod 700 ~/.ssh" "$HADOOP_USER" "$HADOOP_PASSWORD"
    exec_remote "${SLAVE_IPS[$i]}" "[ -f ~/.ssh/id_rsa ] || ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa -q" "$HADOOP_USER" "$HADOOP_PASSWORD"
    exec_remote "${SLAVE_IPS[$i]}" "echo '$master_pubkey' >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys" "$HADOOP_USER" "$HADOOP_PASSWORD"

    local slave_pubkey=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${SLAVE_IPS[$i]} "cat ~/.ssh/id_rsa.pub")
    exec_remote "$MASTER_IP" "echo '$slave_pubkey' >> ~/.ssh/authorized_keys" "$HADOOP_USER" "$HADOOP_PASSWORD"
    echo ""
  done

  # é…ç½® SSH å®¢æˆ·ç«¯
  print_info "é…ç½® SSH å®¢æˆ·ç«¯è®¾ç½®"
  for node_ip in "$MASTER_IP" "${SLAVE_IPS[@]}"; do
    exec_remote "$node_ip" "echo -e 'Host *\n    StrictHostKeyChecking no\n    UserKnownHostsFile=/dev/null' > ~/.ssh/config && chmod 600 ~/.ssh/config" "$HADOOP_USER" "$HADOOP_PASSWORD"
  done

  print_success "SSH æ— å¯†ç ç™»å½•é…ç½®å®Œæˆ"
}

#=============================================================================
# å®‰è£… Hadoop
#=============================================================================

install_hadoop() {
  print_step "æ­¥éª¤ 6: åœ¨ Master èŠ‚ç‚¹å®‰è£… Hadoop"

  # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
  print_progress "æ£€æŸ¥ Master èŠ‚ç‚¹ Hadoop å®‰è£…çŠ¶æ€"
  local hadoop_installed=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${MASTER_IP} \
    "[ -d /usr/local/hadoop ] && [ -f /usr/local/hadoop/bin/hadoop ] && echo 'yes' || echo 'no'")

  if [[ "$hadoop_installed" == "yes" ]]; then
    echo ""

    # è°ƒç”¨å‡½æ•°æ˜¾ç¤ºæç¤ºä¿¡æ¯
    ask_user_choice "MasterèŠ‚ç‚¹å·²å®‰è£…Hadoopï¼Œæ˜¯å¦é‡æ–°å®‰è£…?"

    # æœ¬åœ°è¯»å–ç”¨æˆ·è¾“å…¥
    echo -n "è¯·è¾“å…¥é€‰æ‹© [1/2/3] (é»˜è®¤ 2): "
    read -r user_input
    user_input=${user_input:-2} # é»˜è®¤é€‰æ‹© 2ï¼ˆè·³è¿‡ï¼‰

    # æ•°å­—æ˜ å°„æˆåŠ¨ä½œ
    case "$user_input" in
    1) choice="reinstall" ;;
    2) choice="skip" ;;
    3) choice="exit" ;;
    *) choice="skip" ;; # å…¶ä»–è¾“å…¥é»˜è®¤è·³è¿‡
    esac

    # æ‰§è¡ŒåŠ¨ä½œ
    case "$choice" in
    "exit")
      print_warning "ç”¨æˆ·é€‰æ‹©é€€å‡º"
      exit 0
      ;;
    "reinstall")
      print_info "é‡æ–°å®‰è£… Hadoop"
      exec_remote "$MASTER_IP" "sudo rm -rf /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"
      ;;
    "skip")
      print_info "è·³è¿‡ Hadoop å®‰è£…ï¼Œä½¿ç”¨ç°æœ‰ç‰ˆæœ¬"
      return 0
      ;;
    esac
  fi

  local dir="$(dirname "$0")"
  local local_pkg="$dir/hadoop-${HADOOP_VERSION}.tar.gz"

  # æ£€æŸ¥æœ¬åœ°å®‰è£…åŒ…
  if [[ -f "$local_pkg" ]]; then
    print_info "æ£€æµ‹åˆ°æœ¬åœ° Hadoop å®‰è£…åŒ…"
    echo -n "æ˜¯å¦ä½¿ç”¨æœ¬åœ°å®‰è£…åŒ…? (y/N): "
    read -r use_local
    if [[ "$use_local" =~ ^[Yy]$ ]]; then
      print_info "ä¸Šä¼ æœ¬åœ° Hadoop å®‰è£…åŒ…åˆ° Master èŠ‚ç‚¹"
      sshpass -p "$HADOOP_PASSWORD" rsync -avz --progress -e "ssh -o StrictHostKeyChecking=no" \
        "$local_pkg" ${HADOOP_USER}@${MASTER_IP}:/tmp/ 2>&1 | grep -v "sending incremental" || true
    else
      print_info "ä»é•œåƒæºä¸‹è½½ Hadoop"
      exec_remote "$MASTER_IP" "cd /tmp && wget -q --show-progress https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz || wget -q --show-progress http://mirrors.cloud.aliyuncs.com/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz" "$HADOOP_USER" "$HADOOP_PASSWORD"
    fi
  else
    print_info "ä»é•œåƒæºä¸‹è½½ Hadoop"
    exec_remote "$MASTER_IP" "cd /tmp && wget --show-progress https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz || wget --show-progress http://mirrors.cloud.aliyuncs.com/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz" "$HADOOP_USER" "$HADOOP_PASSWORD"
  fi

  print_info "è§£å‹å¹¶å®‰è£… Hadoop"
  exec_remote "$MASTER_IP" "cd /tmp && tar -zxf hadoop-${HADOOP_VERSION}.tar.gz" "$HADOOP_USER" "$HADOOP_PASSWORD"
  exec_remote "$MASTER_IP" "sudo mv /tmp/hadoop-${HADOOP_VERSION} /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"
  exec_remote "$MASTER_IP" "sudo chown -R $HADOOP_USER:$HADOOP_USER /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"

  print_info "é…ç½® Hadoop ç¯å¢ƒå˜é‡"
  exec_remote "$MASTER_IP" "grep -q 'HADOOP_HOME' ~/.bashrc || cat >> ~/.bashrc << 'ENVEOF'
export JAVA_HOME=/usr/lib/jvm/jdk-17.0.12-oracle-x64
export HADOOP_HOME=/usr/local/hadoop
export PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin
export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop
ENVEOF" "$HADOOP_USER" "$HADOOP_PASSWORD"

  print_success "Hadoop å®‰è£…å®Œæˆ"
}

#=============================================================================
# é…ç½®Hadoopé›†ç¾¤æ–‡ä»¶
#=============================================================================

configure_hadoop_files() {
  print_step "æ­¥éª¤ 7: é…ç½® Hadoop é›†ç¾¤æ–‡ä»¶"

  print_progress "é…ç½® hadoop-env.sh"
  exec_remote "$MASTER_IP" "sed -i 's|# export JAVA_HOME=.*|export JAVA_HOME=/usr/lib/jvm/jdk-17.0.12-oracle-x64|g' /usr/local/hadoop/etc/hadoop/hadoop-env.sh" "$HADOOP_USER" "$HADOOP_PASSWORD"
  echo ""

  print_progress "é…ç½® workers æ–‡ä»¶"
  local workers_content=""
  for hostname in "${SLAVE_HOSTNAMES[@]}"; do
    workers_content+="$hostname"$'\n'
  done
  exec_remote "$MASTER_IP" "echo '$workers_content' | tee /usr/local/hadoop/etc/hadoop/workers > /dev/null" "$HADOOP_USER" "$HADOOP_PASSWORD"
  echo ""

  print_progress "é…ç½® core-site.xml"
  exec_remote "$MASTER_IP" "cat > /usr/local/hadoop/etc/hadoop/core-site.xml << 'XMLEOF'
<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://${MASTER_HOSTNAME}:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>${HADOOP_USER}</value>
    </property>
</configuration>
XMLEOF
" "$HADOOP_USER" "$HADOOP_PASSWORD"
  echo ""

  print_progress "é…ç½® hdfs-site.xml"
  exec_remote "$MASTER_IP" "cat > /usr/local/hadoop/etc/hadoop/hdfs-site.xml << 'XMLEOF'
<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>${SLAVE_COUNT}</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>${MASTER_HOSTNAME}:9870</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>${MASTER_HOSTNAME}:9868</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>
XMLEOF
" "$HADOOP_USER" "$HADOOP_PASSWORD"
  echo ""

  print_progress "é…ç½® yarn-site.xml"
  exec_remote "$MASTER_IP" "cat > /usr/local/hadoop/etc/hadoop/yarn-site.xml << 'XMLEOF'
<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>${MASTER_HOSTNAME}</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>${MASTER_HOSTNAME}:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
XMLEOF
" "$HADOOP_USER" "$HADOOP_PASSWORD"
  echo ""

  print_progress "é…ç½® mapred-site.xml"
  exec_remote "$MASTER_IP" "cat > /usr/local/hadoop/etc/hadoop/mapred-site.xml << 'XMLEOF'
<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>${MASTER_HOSTNAME}:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>${MASTER_HOSTNAME}:19888</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>\$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:\$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
XMLEOF
" "$HADOOP_USER" "$HADOOP_PASSWORD"
  echo ""

  exec_remote "$MASTER_IP" "chown -R $HADOOP_USER:$HADOOP_USER /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"

  print_success "Hadoop é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ"
}

#=============================================================================
# åˆ†å‘Hadoopåˆ°æ‰€æœ‰SlaveèŠ‚ç‚¹
#=============================================================================

distribute_hadoop() {
  print_step "æ­¥éª¤ 8: åˆ†å‘ Hadoop åˆ°æ‰€æœ‰ Slave èŠ‚ç‚¹"

  print_info "åœ¨ Master èŠ‚ç‚¹æ‰“åŒ… Hadoop"
  exec_remote "$MASTER_IP" "cd /usr/local && tar -zcf /tmp/hadoop.tar.gz hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"

  local total_slaves=$SLAVE_COUNT
  local current=0

  for i in $(seq 0 $((SLAVE_COUNT - 1))); do
    current=$((current + 1))
    show_progress $current $total_slaves "åˆ†å‘åˆ° ${SLAVE_HOSTNAMES[$i]}"

    # æ£€æŸ¥ç›®æ ‡èŠ‚ç‚¹æ˜¯å¦å·²æœ‰Hadoop
    local slave_has_hadoop=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${SLAVE_IPS[$i]} \
      "[ -d /usr/local/hadoop ] && echo 'yes' || echo 'no'")

    if [[ "$slave_has_hadoop" == "yes" ]]; then
      exec_remote "${SLAVE_IPS[$i]}" "rm -rf /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"
    fi

    # ä½¿ç”¨ Master ä¸Šçš„ hadoop ç”¨æˆ·ç›´æ¥ scp
    exec_remote "$MASTER_IP" "scp -o StrictHostKeyChecking=no /tmp/hadoop.tar.gz ${SLAVE_HOSTNAMES[$i]}:/tmp/" "$HADOOP_USER" "$HADOOP_PASSWORD"

    # åœ¨ Slave èŠ‚ç‚¹è§£å‹
    exec_remote "${SLAVE_IPS[$i]}" "cd /tmp && tar -zxf hadoop.tar.gz && sudo mv hadoop /usr/local/ && sudo chown -R $HADOOP_USER:$HADOOP_USER /usr/local/hadoop" "$HADOOP_USER" "$HADOOP_PASSWORD"

    # é…ç½®ç¯å¢ƒå˜é‡
    exec_remote "${SLAVE_IPS[$i]}" "grep -q 'HADOOP_HOME' ~/.bashrc || cat >> ~/.bashrc << 'ENVEOF'
export JAVA_HOME=/usr/lib/jvm/jdk-17.0.12-oracle-x64
export HADOOP_HOME=/usr/local/hadoop
export PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin
export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop
ENVEOF
" "$HADOOP_USER" "$HADOOP_PASSWORD"

    echo ""
  done

  print_success "Hadoop åˆ†å‘å®Œæˆ"
}

#=============================================================================
# å¯åŠ¨é›†ç¾¤
#=============================================================================

start_cluster() {
  print_step "æ­¥éª¤ 9: æ ¼å¼åŒ– NameNode å¹¶å¯åŠ¨é›†ç¾¤"

  # æ£€æŸ¥æ˜¯å¦å·²ç»æ ¼å¼åŒ–
  print_progress "æ£€æŸ¥ NameNode æ˜¯å¦å·²æ ¼å¼åŒ–"
  local namenode_formatted=$(sshpass -p "$HADOOP_PASSWORD" ssh -o StrictHostKeyChecking=no ${HADOOP_USER}@${MASTER_IP} \
    "[ -d /usr/local/hadoop/tmp/dfs/name/current ] && echo 'yes' || echo 'no'")

  if [[ "$namenode_formatted" == "yes" ]]; then
    echo ""
    print_warning "æ£€æµ‹åˆ° NameNode å·²ç»æ ¼å¼åŒ–è¿‡"

    # æ˜¾ç¤ºæç¤º
    ask_user_choice "NameNodeå·²æ ¼å¼åŒ–ï¼Œæ˜¯å¦é‡æ–°æ ¼å¼åŒ–? (é‡æ–°æ ¼å¼åŒ–ä¼šæ¸…ç©ºHDFSæ•°æ®!)"

    # æœ¬åœ°è¯»å–ç”¨æˆ·è¾“å…¥
    echo -n "è¯·è¾“å…¥é€‰æ‹© [1/2/3] (é»˜è®¤ 2): "
    read -r user_input
    user_input=${user_input:-2} # é»˜è®¤é€‰æ‹© 2ï¼ˆè·³è¿‡ï¼‰

    # æ•°å­—æ˜ å°„æˆåŠ¨ä½œ
    case "$user_input" in
    1) choice="reinstall" ;;
    2) choice="skip" ;;
    3) choice="exit" ;;
    *) choice="skip" ;;
    esac

    # æ‰§è¡ŒåŠ¨ä½œ
    case "$choice" in
    "exit")
      print_warning "ç”¨æˆ·é€‰æ‹©é€€å‡º"
      exit 0
      ;;
    "reinstall")
      print_info "é‡æ–°æ ¼å¼åŒ– NameNode"
      exec_remote "$MASTER_IP" "rm -rf /usr/local/hadoop/tmp/dfs/name/*" "$HADOOP_USER" "$HADOOP_PASSWORD"
      exec_remote "$MASTER_IP" "source ~/.bashrc && hdfs namenode -format -force" "$HADOOP_USER" "$HADOOP_PASSWORD"
      ;;
    "skip")
      print_info "è·³è¿‡æ ¼å¼åŒ–æ­¥éª¤"
      ;;
    esac
  else
    echo ""
    print_info "æ ¼å¼åŒ– NameNode"
    exec_remote "$MASTER_IP" "source ~/.bashrc && hdfs namenode -format -force" "$HADOOP_USER" "$HADOOP_PASSWORD"
  fi

  print_info "å¯åŠ¨ HDFS"
  exec_remote "$MASTER_IP" "source ~/.bashrc && start-dfs.sh" "$HADOOP_USER" "$HADOOP_PASSWORD"
  sleep 5

  print_info "å¯åŠ¨ YARN"
  exec_remote "$MASTER_IP" "source ~/.bashrc && start-yarn.sh" "$HADOOP_USER" "$HADOOP_PASSWORD"
  sleep 5

  print_info "å¯åŠ¨ JobHistoryServer"
  exec_remote "$MASTER_IP" "source ~/.bashrc && mapred --daemon start historyserver" "$HADOOP_USER" "$HADOOP_PASSWORD"
  sleep 3

  print_success "é›†ç¾¤å¯åŠ¨å®Œæˆ"
}

#=============================================================================
# éªŒè¯é›†ç¾¤
#=============================================================================

verify_cluster() {
  print_step "æ­¥éª¤ 10: éªŒè¯é›†ç¾¤çŠ¶æ€"

  print_info "æ£€æŸ¥ Master èŠ‚ç‚¹è¿›ç¨‹:"
  exec_remote "$MASTER_IP" "source ~/.bashrc && jps" "$HADOOP_USER" "$HADOOP_PASSWORD"

  echo ""
  print_info "æ£€æŸ¥ HDFS çŠ¶æ€:"
  exec_remote "$MASTER_IP" "source ~/.bashrc && hdfs dfsadmin -report" "$HADOOP_USER" "$HADOOP_PASSWORD"

  echo ""
  print_info "æ£€æŸ¥ YARN èŠ‚ç‚¹:"
  exec_remote "$MASTER_IP" "source ~/.bashrc && yarn node -list" "$HADOOP_USER" "$HADOOP_PASSWORD"

  echo -e "\n${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
  echo -e "${GREEN}â•‘             é›†ç¾¤éƒ¨ç½²å®Œæˆ!                                  â•‘${NC}"
  echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"

  echo -e "${YELLOW}ğŸ“Š Web è®¿é—®åœ°å€:${NC}"
  echo -e "  ${CYAN}NameNode WebUI:${NC}        http://${MASTER_IP}:9870"
  echo -e "  ${CYAN}ResourceManager WebUI:${NC} http://${MASTER_IP}:8088"
  echo -e "  ${CYAN}JobHistory WebUI:${NC}      http://${MASTER_IP}:19888"
  echo ""

  echo -e "${YELLOW}ğŸ” SSH ç™»å½• Master èŠ‚ç‚¹:${NC}"
  echo -e "  ${CYAN}ssh $HADOOP_USER@$MASTER_IP${NC}"
  echo ""

  echo -e "${YELLOW}ğŸ›‘ åœæ­¢é›†ç¾¤å‘½ä»¤ (åœ¨ Master èŠ‚ç‚¹ä»¥ hadoop ç”¨æˆ·æ‰§è¡Œ):${NC}"
  echo -e "  ${CYAN}mapred --daemon stop historyserver${NC}"
  echo -e "  ${CYAN}stop-yarn.sh${NC}"
  echo -e "  ${CYAN}stop-dfs.sh${NC}"
  echo ""

  echo -e "${YELLOW}ğŸ§ª æµ‹è¯• MapReduce ç¤ºä¾‹:${NC}"
  echo -e "  ${CYAN}hdfs dfs -mkdir -p /user/hadoop/input${NC}"
  echo -e "  ${CYAN}hdfs dfs -put \$HADOOP_HOME/etc/hadoop/*.xml /user/hadoop/input${NC}"
  echo -e "  ${CYAN}hadoop jar \$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-${HADOOP_VERSION}.jar wordcount /user/hadoop/input /user/hadoop/output${NC}"
  echo -e "  ${CYAN}hdfs dfs -cat /user/hadoop/output/part-r-00000${NC}"
  echo ""
}

#=============================================================================
# ä¸»å‡½æ•°
#=============================================================================

main() {
  clear
  echo -e "${GREEN}"
  cat <<"EOF"
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     Hadoop é›†ç¾¤è‡ªåŠ¨éƒ¨ç½²è„šæœ¬ (ä¿®å¤ç‰ˆ)                      â•‘
    â•‘     Hadoop 3.4.2 + Ubuntu 24.04 + OpenJDK 17              â•‘
    â•‘     ç‰ˆæœ¬: v2.0 - å®Œæ•´æ£€æµ‹ä¸è¿›åº¦æç¤º                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF
  echo -e "${NC}"

  # æ£€æŸ¥å¿…è¦å·¥å…·
  print_info "æ£€æŸ¥å¿…è¦å·¥å…·..."
  if ! command -v sshpass &>/dev/null; then
    print_info "å®‰è£… sshpass..."
    sudo apt update && sudo apt install -y sshpass
  fi

  if ! command -v rsync &>/dev/null; then
    print_info "å®‰è£… rsync..."
    sudo apt update && sudo apt install -y rsync
  fi

  print_success "å¿…è¦å·¥å…·æ£€æŸ¥å®Œæˆ"

  # æ‰§è¡Œéƒ¨ç½²æµç¨‹
  collect_cluster_info
  check_node_connectivity
  configure_all_nodes
  configure_hosts_file
  configure_ssh_keys
  install_hadoop
  configure_hadoop_files
  distribute_hadoop
  start_cluster
  verify_cluster

  print_success "æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ!"
}

# æ•è·é”™è¯¯
trap 'print_error "è„šæœ¬æ‰§è¡Œå‡ºé”™ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯"; exit 1' ERR

# è¿è¡Œä¸»å‡½æ•°
main "$@"
